{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.复习课上内容， 阅读相应论文。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 回答以下理论题目"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.  What is autoencoder?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An autoencoder is an unsupervised machine learning algorithm that takes an image as input and reconstructs it using fewer number of bits. That may sound like image compression, but the biggest difference between an autoencoder and a general purpose image compression algorithms is that in case of autoencoders, the compression is achieved by learning on a training set of data. While reasonable compression is achieved when an image is similar to the training set used, autoencoders are poor general-purpose image compressors; JPEG compression will do vastly better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. What are the differences between greedy search and beam search?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "贪心算法在翻译每个字的时候，直接选择条件概率最大的候选值作为当前最优\n",
    "beam search是对greedy search的一个改进算法。相对greedy search扩大了搜索空间，但远远不及穷举搜索指数级的搜索空间，是二者的一个折中方案。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. What is the intuition of attention mechanism?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当预测一个序列中的单词时，没有必要看输入中具有相同重要性的所有单词。注意机制主要集中在某些部分，两个相距甚远的词之间存在直接联系"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. What is the disadvantage of word embeding introduced in previous lectures ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不能解决一次多义问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. What is the architecture of ELMo model. (A brief description is enough)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "char-level+多层BLM的组合，再将输出向量整合为权值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Compared to RNN,  what is the advantage of Transformer ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "突破了 RNN 模型不能并行计算的限制"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Why we use layer normalizaiton instead of batch normalization in Transformer ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batch是“竖”着来的，各个维度做归一化，所以与batch size有关系。\n",
    "layer是“横”着来的，对一个样本，不同的神经元neuron间做归一化。\n",
    "layer可以并行计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Why we need position embedding in Transformer ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transformer不能很好的反映语序"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Briefly describe what is self-attention and what is multi-head attention?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "self-attention 通过一个FNN矩阵后加权求和得到最终结果\n",
    "\n",
    "\n",
    "multi-head attention 通过多个FNN矩阵多个加权求和得到最终结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. What is the basic unit of GPT model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12-layer decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11. Briefly descibe how to use GPT in other NLP tasks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT采用了单项的Transformer完成预训练任务，并且将12个Trm叠加起来，而训练的过程其实非常的简单，就是将句子n个词的词向量(第一个为<SOS>)加上Positional Encoding后输入到前面提到的Transfromer中，n个输出分别预测该位置的下一个词(<SOS>预测句子中的第一个词，最后一个词的预测结果不用于语言模型的训练)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12. What is masked language model in BERT ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bert中的屏蔽语言模型是一种双向转换器，可屏蔽一个单词，以避免直接被模型复制。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13. What are the inputs of BERT ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1)词嵌入后的Token Embedding，每次输入总以符号[CLS]的embedding开始，如果是两个句子，则句之间用[SEP]隔开。\n",
    "\n",
    "2)句子类别的符号\n",
    "\n",
    "3)Position Embedding，这个与Transformer中的一致。\n",
    "\n",
    "上述三个向量相加，组成BERT的输入。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 14. Briely descibe how to use BERT in other NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT应用在NLP诸多任务中，诸如文本分类、文本相似度、问答系统、文本标记如词性POS命名和实体识别NER等"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15. What are the differences between these three models: GPT, BERT, GPT2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT:GPT训练任务使用的是单向语言模型，特征提取器使用的是Transformer，GPT预训练模型取自Transformer的decoder阶段，使用了Masked Multi-Head Attention，GPT在预训练阶段是无监督学习，通过大量预料进行训练，Fine-tuning阶段是有监督学习，GPT无监督预训练过程，单向语言模型通过上文预测当前词\n",
    "\n",
    "GPT2:相比GPT做了以下变化\n",
    "(1) Layer normalization被移到了sub-block之前\n",
    "\n",
    "(2) 缩放残差层的权重\n",
    "\n",
    "(3) 词表被扩大到50257、context size从512扩大到1024、batchsize使用512\n",
    "\n",
    "(4) transformer层数使用的是48层\n",
    "\n",
    "BERT：预训练使用的是双向语言模型，Bert还有突出的地方在于它的预训练任务上，Bert采用了两个预训练任务：Masked语言模型(本质上是CBOW)、Next Sentence Prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
